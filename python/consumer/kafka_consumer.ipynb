{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Kafka Consumer\n",
        "\n",
        "| Nama                    | NRP        |\n",
        "| ----------------------- | ---------- |\n",
        "| Dwiyasa Nakula   | 5027221001 |\n",
        "| Muhammad Afif | 5027221032 |"
      ],
      "metadata": {
        "id": "7y8SJfoZmgbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import from_json, col, concat, lit\n",
        "from pyspark.sql.types import StructType, StringType, FloatType"
      ],
      "metadata": {
        "id": "a4_wFWbEmf2r"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AtBzYJ_omFqQ"
      },
      "outputs": [],
      "source": [
        "# Step 1: Configure Spark session for local execution\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SensorDataProcessor\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0\") \\\n",
        "    .config(\"spark.master\", \"local[*]\") \\\n",
        "    .config(\"spark.driver.host\", \"localhost\") \\\n",
        "    .config(\"spark.driver.bindAddress\", \"localhost\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable verbose logging\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")"
      ],
      "metadata": {
        "id": "lLqB5Bqgml-w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Create Kafka stream\n",
        "sensor_data = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"157.245.61.228:9092\") \\\n",
        "    .option(\"subscribe\", \"iot-suhu\") \\\n",
        "    .load()"
      ],
      "metadata": {
        "id": "uRWUSAp9mnBq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define schema for JSON data\n",
        "schema = StructType() \\\n",
        "    .add(\"sensor_id\", StringType()) \\\n",
        "    .add(\"suhu\", FloatType())"
      ],
      "metadata": {
        "id": "vDIN9BjJmpn-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_data = spark \\\n",
        "    .read \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"157.245.61.228:9092\") \\\n",
        "    .option(\"subscribe\", \"iot-suhu\") \\\n",
        "    .load() \\\n",
        "    .selectExpr(\"CAST(value AS STRING) as json\") \\\n",
        "    .select(from_json(col(\"json\"), schema).alias(\"data\")) \\\n",
        "    .select(\"data.sensor_id\", \"data.suhu\")\n",
        "\n",
        "# Show a small sample\n",
        "batch_data.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ms_ogi3wpAEj",
        "outputId": "1fea82ca-b28e-43f5-a968-a6b4838b8a30"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|sensor_id| suhu|\n",
            "+---------+-----+\n",
            "|       S3|53.34|\n",
            "|       S3|97.43|\n",
            "|       S2|64.49|\n",
            "|       S3|74.17|\n",
            "|       S1|89.61|\n",
            "|       S3|94.21|\n",
            "|       S2|90.11|\n",
            "|       S2|62.46|\n",
            "|       S1|64.48|\n",
            "|       S2|54.66|\n",
            "+---------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tampilkan Data perolehan yang difilter"
      ],
      "metadata": {
        "id": "5BQsCnKC2z80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Filter batch data\n",
        "alert_df = batch_data.filter(batch_data.suhu > 80)\n",
        "alert_df = alert_df.withColumn(\"suhu\", concat(col(\"suhu\").cast(\"string\"), lit(\"°C\")))\n",
        "\n",
        "# Show the alerts\n",
        "alert_df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot23kWZK1G3o",
        "outputId": "13dbbfe4-5cff-4fc8-b7cb-5351f2c27d78"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+\n",
            "|sensor_id|   suhu|\n",
            "+---------+-------+\n",
            "|       S3|97.43°C|\n",
            "|       S1|89.61°C|\n",
            "|       S3|94.21°C|\n",
            "|       S2|90.11°C|\n",
            "|       S1|93.02°C|\n",
            "|       S1|80.06°C|\n",
            "|       S1|95.39°C|\n",
            "|       S1|90.36°C|\n",
            "|       S3|86.37°C|\n",
            "|       S2|99.39°C|\n",
            "+---------+-------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cara Alternatif\n",
        "\n",
        "Gunakan kode berikut pada Step 4 apabila menggunakan pyspark local. Google Colab dan Kaggle tidak mendukung tampilan data streaming"
      ],
      "metadata": {
        "id": "xFfmKWto15gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Step 4: Convert Kafka value to structured data\n",
        "# sensor_df = sensor_data \\\n",
        "#     .selectExpr(\"CAST(value AS STRING) as json\") \\\n",
        "#     .select(from_json(col(\"json\"), schema).alias(\"data\")) \\\n",
        "#     .select(\"data.sensor_id\", \"data.suhu\")\n",
        "\n",
        "# # Filter for temperatures above 80°C and add \"°C\" suffix\n",
        "# alert_df = sensor_df.filter(sensor_df.suhu > 80)\n",
        "# alert_df = alert_df.withColumn(\"suhu\", concat(col(\"suhu\").cast(\"string\"), lit(\"°C\")))"
      ],
      "metadata": {
        "id": "6mZRYW84ms-y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import time"
      ],
      "metadata": {
        "id": "LVi_fZi0orTS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Step 5: Display alerts in console every second\n",
        "# query = alert_df \\\n",
        "#     .writeStream \\\n",
        "#     .outputMode(\"append\") \\\n",
        "#     .format(\"console\") \\\n",
        "#     .option(\"truncate\", \"false\") \\\n",
        "#     .trigger(processingTime=\"1 second\") \\\n",
        "#     .start()\n",
        "\n",
        "# # Gunakan await termination apabila ingin terus dijalankan, hentikan dengan mematikan sel\n",
        "\n",
        "# query.awaitTermination()\n",
        "\n",
        "# # Gunakan time apabila hanya ingin dijalankan selama waktu tertentu\n",
        "\n",
        "# time.sleep(30)\n",
        "\n",
        "# # Stop the query after the sleep period\n",
        "# query.stop()"
      ],
      "metadata": {
        "id": "TJZxzDmRm3OC"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}